{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx6T4HmZk3Uf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2LqjHW3l5nm"
      },
      "outputs": [],
      "source": [
        "# Data Path and Configurations\n",
        "dataset_dir = r\"/dataset\"\n",
        "save_path = r'/vgg16_results.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxyx5nw_7hws"
      },
      "outputs": [],
      "source": [
        "# Initialize results by loading existing data if available\n",
        "def load_existing_results(path):\n",
        "    try:\n",
        "        with open(path, 'r') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        return []\n",
        "\n",
        "# Load results\n",
        "all_results = load_existing_results(save_path)\n",
        "\n",
        "# Adjusted save_results function\n",
        "def save_results(metrics):\n",
        "    # Load existing results if any\n",
        "    try:\n",
        "        with open(save_path, 'r') as f:\n",
        "            all_results = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        all_results = []\n",
        "\n",
        "    # Append new results\n",
        "    all_results.append(metrics)\n",
        "\n",
        "    # Save updated results to file\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(all_results, f, indent=4)\n",
        "    print(f\"Results saved successfully with metrics: {metrics}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp90-5Avl4Jn"
      },
      "outputs": [],
      "source": [
        "def get_data_generators(data_dir, batch_size, img_size=(224, 224), val_split=0.2):\n",
        "    \"\"\"\n",
        "    Function to create and return training and validation generators.\n",
        "    \"\"\"\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        brightness_range=[0.8, 1.2],\n",
        "        zoom_range=[0.8, 1.2],\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest',\n",
        "        channel_shift_range=20.0,      # Random channel shifts\n",
        "        vertical_flip=True,            # Flip images vertically\n",
        "        validation_split=val_split\n",
        "    )\n",
        "\n",
        "    train_gen = datagen.flow_from_directory(\n",
        "        data_dir, target_size=img_size, batch_size=batch_size,\n",
        "        class_mode='categorical', subset='training'\n",
        "    )\n",
        "\n",
        "    val_gen = datagen.flow_from_directory(\n",
        "        data_dir, target_size=img_size, batch_size=batch_size,\n",
        "        class_mode='categorical', subset='validation'\n",
        "    )\n",
        "\n",
        "    return train_gen, val_gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGWMa5e6l1mc"
      },
      "outputs": [],
      "source": [
        "def build_vgg16_model(learning_rate, dropout_rate=None, activation_function='relu', optimizer='adam'):\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False  # Freeze all layers\n",
        "\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(256, activation=activation_function)(x)\n",
        "    if dropout_rate is not None:\n",
        "        x = Dropout(dropout_rate)(x)  # Apply dropout if dropout_rate is specified\n",
        "    x = Dense(128, activation=activation_function)(x)\n",
        "    if dropout_rate is not None:\n",
        "        x = Dropout(dropout_rate)(x)  # Apply dropout if dropout_rate is specified\n",
        "    output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "    # Choose optimizer based on parameter\n",
        "    if optimizer == 'adam':\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer: {optimizer}\")\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            AUC(name='auc'),\n",
        "            Precision(name='precision'),\n",
        "            Recall(name='recall')\n",
        "        ]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hyqo1HNlqIqo"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_model(learning_rate, batch_size, dropout_rate, activation_function, optimizer):\n",
        "    # Load data generators\n",
        "    train_gen, val_gen = get_data_generators(dataset_dir, batch_size, img_size=(224, 224))\n",
        "\n",
        "    # Build and compile the model\n",
        "    model = build_vgg16_model(learning_rate, dropout_rate, activation_function, optimizer)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
        "\n",
        "    # Train the model and get training history\n",
        "    history = model.fit(train_gen, validation_data=val_gen, epochs=30, callbacks=[early_stopping])\n",
        "\n",
        "    # Evaluate on validation data\n",
        "    val_metrics = model.evaluate(val_gen)\n",
        "\n",
        "    # Unpack the metrics\n",
        "    val_loss = val_metrics[0]\n",
        "    val_accuracy = val_metrics[1]\n",
        "    val_auc = val_metrics[2]\n",
        "    val_precision = val_metrics[3]\n",
        "    val_recall = val_metrics[4]\n",
        "    val_predictions = model.predict(val_gen)\n",
        "\n",
        "    # Extract final training metrics from history\n",
        "    train_loss = history.history['loss'][-1]\n",
        "    train_accuracy = history.history['accuracy'][-1]\n",
        "\n",
        "    # Save the model with a unique name based on hyperparameters\n",
        "    model_name = f\"vgg16_lr{learning_rate}_bs{batch_size}_dr{dropout_rate}_{activation_function}_{optimizer}.h5\"\n",
        "    save_dir = os.path.join(\"/saved_models\", model_name)\n",
        "    model.save(save_dir)\n",
        "    print(f\"Model saved to {save_dir}\")\n",
        "\n",
        "    # Return a dictionary of metrics and model details\n",
        "    return {\n",
        "        'train_loss': train_loss,\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'val_loss': val_loss,\n",
        "        'val_accuracy': val_accuracy,\n",
        "        'val_auc': val_auc,\n",
        "        'model_name': model_name,\n",
        "        'hyperparameters': {\n",
        "            'learning_rate': learning_rate,\n",
        "            'batch_size': batch_size,\n",
        "            'dropout_rate': dropout_rate,\n",
        "            'activation_function': activation_function,\n",
        "            'optimizer': optimizer\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FYHiBX3AId8"
      },
      "outputs": [],
      "source": [
        "# Define parameter dictionary with fixed values for each hyperparameter\n",
        "param_dict = {\n",
        "    'learning_rate': 0.001,        # Fixed learning rate\n",
        "    'batch_size': 32,              # Fixed batch size\n",
        "    'dropout_rate': 0.3,           # Fixed dropout rate\n",
        "    'activation_function': 'leaky_relu', # Fixed activation function\n",
        "    'optimizer': 'sgd'            # Fixed optimizer\n",
        "}\n",
        "\n",
        "# Example usage: passing `param_dict` to train_and_evaluate_model\n",
        "metrics = train_and_evaluate_model(**param_dict)\n",
        "\n",
        "# Print the metrics to verify\n",
        "print(metrics)\n",
        "\n",
        "# Save results with parameters and metrics\n",
        "save_results(metrics)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}